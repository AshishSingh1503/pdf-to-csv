# Server-side example environment variables (copy to server/.env or top-level .env)
# Tune these for your environment; defaults are provided in code when not set.

# Server
PORT=5000
NODE_ENV=development
LOG_LEVEL=info            # debug|info|warn|error
WS_PATH=/ws

# Database / Postgres (Cloud SQL friendly)
DB_HOST=<your_db_host>
DB_PORT=5432
DB_NAME=<your_db_name>
DB_USER=<your_db_user>
DB_PASSWORD=<your_db_password>
DB_SSL=false
# Connection pool sizing - optimized for high-resource environment (8 vGPU/64GB)
# Increase to 500 for aggressive parallel processing with Cloud SQL
DB_POOL_MAX=500
DB_POOL_MIN=2

# Worker & cache tuning
# WORKER_THREAD_POOL_SIZE - worker threads for CPU-bound validation tasks.
# Set to 16 for high-resource environment (8 vGPU/64GB). Adjust based on available CPU cores.
WORKER_THREAD_POOL_SIZE=16
# CACHE_TTL seconds for node-cache
CACHE_TTL=300

# Bulk insert chunk size - optimized for high-resource environment (8 vGPU/64GB)
DB_INSERT_CHUNK_SIZE=5000

# ==============================
# Batch Queue Configuration
# ==============================
# Average batch processing time in seconds for ETA estimation (default: 150 = 2.5 minutes)
AVERAGE_BATCH_SECONDS=150
# Maximum number of batches to process concurrently (default: 2)
# Recommended for high-resource environments: 20
MAX_CONCURRENT_BATCHES=20
# Timeout for batch processing in milliseconds (code default: 1800000 = 30 minutes)
# For high-resource environments with very large batches, adjust accordingly.
# BATCH_QUEUE_TIMEOUT=1800000
# Optional multiplier (0.5 - 5.0) applied to the base BATCH_QUEUE_TIMEOUT in code.
# Use to scale timeout dynamically for exceptionally large batches without changing the base value.
# BATCH_QUEUE_TIMEOUT_MULTIPLIER=1.0
# Enable verbose queue operation logging (default: false, use for debugging)
ENABLE_QUEUE_LOGGING=false

# Maximum number of queued batches to retain before rejecting new uploads. Helps prevent OOM under heavy load.
# Default: 100 (high-performance: 500)
MAX_QUEUE_LENGTH=500

# Document AI Worker Scaling (configured in code, not via env var):
# - 1 file: 4 workers | 2 files: 8 workers | ≤10 files: 10 workers
# - ≤30 files: up to 50 workers | ≤100 files: 80 workers | >100 files: 120 workers
# - Maximum capacity: 150 concurrent Document AI requests
# Controls whether the server should attempt a graceful shutdown by waiting for active batches to finish.
# If unset the default is enabled. Set to 'false' to disable graceful shutdown behavior.
ENABLE_GRACEFUL_SHUTDOWN=true
# How long (ms) to wait for active batches during graceful shutdown before forcing exit. Default: 300000 (5 minutes)
GRACEFUL_SHUTDOWN_TIMEOUT=300000

# Logging rotation options are controlled by winston config (logs/ directory)

# Google / Document AI
GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_json>
PROJECT_ID=<your_gcp_project_id>
LOCATION=us
PROCESSOR_ID=<your_document_ai_processor_id>

# Per-request timeout for Document AI calls (milliseconds)
# Controls how long we wait for the Document AI processDocument call before treating it as a timeout.
# Default (code): 1200000 (20 minutes). Minimum supported: 60000 (1 minute).
# REQUEST_TIMEOUT_MS=1200000

# Storage
INPUT_BUCKET=<your_input_bucket>
OUTPUT_BUCKET=<your_output_bucket>

# Feature toggles
ENABLE_DUPLICATE_DETECTION=true
DUPLICATE_KEY_FIELD=mobile

# Admin API key for /api/admin endpoints (internal use only)
# Set a strong random value in production. Example: ADMIN_API_KEY=changeme
ADMIN_API_KEY=
